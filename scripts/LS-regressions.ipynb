{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "from scipy.stats import hmean\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files is 5\n"
     ]
    }
   ],
   "source": [
    "src_path = '../data/'\n",
    "file_types = ('.csv')\n",
    " \n",
    "file_paths = []  \n",
    "for root, dirs, files in os.walk(src_path):\n",
    "    file_paths.extend([os.path.join(root, f) for f in files if f.endswith(file_types)])\n",
    "    \n",
    "print 'number of files is', len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/LS/LS-L2-biomarkers.csv',\n",
       " '../data/LS/LS-L3-biomarkers.csv',\n",
       " '../data/LS/LS-L4-biomarkers.csv',\n",
       " '../data/LS/LS-L5-biomarkers.csv',\n",
       " '../data/LS/LS-L6-biomarkers.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biomarkers = [\"Stool.Lysozyme\",\n",
    "              \"Stool.Lactoferrin\", \n",
    "              \"Stool.SIgA\",\n",
    "              \"X..SCFA.Acetate\",\n",
    "              \"X..SCFA.Propionate\",\n",
    "              \"X..SCFA.Valerate\",\n",
    "              \"X..SCFA.Butyrate\",\n",
    "              \"Total.SCFA\",\n",
    "              \"Butyrate\",\n",
    "              \"Stool.pH\",\n",
    "              \"Neutrophil.Count\",\n",
    "              \"Lymphocyte.Count\" ,\n",
    "              \"Monocyte.Count\",\n",
    "              \"Esoinophil.Count\"]\n",
    "\n",
    "\n",
    "biomarker = biomarkers[1]\n",
    "#biomarker = \"Neutrophil.Count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Stool.Lysozyme\n",
      "working on Stool.Lactoferrin\n",
      "working on Stool.SIgA\n",
      "working on X..SCFA.Acetate\n",
      "working on X..SCFA.Propionate\n",
      "working on X..SCFA.Valerate\n",
      "working on X..SCFA.Butyrate\n",
      "working on Total.SCFA\n",
      "working on Butyrate\n",
      "working on Stool.pH\n",
      "working on Neutrophil.Count\n",
      "working on Lymphocyte.Count\n",
      "working on Monocyte.Count\n",
      "working on Esoinophil.Count\n"
     ]
    }
   ],
   "source": [
    "scoring_metric = \"mean_absolute_error\"\n",
    "\n",
    "\n",
    "def CV_scores(regressor, X, y, data_type, estimator_name):\n",
    "    y_n = (y - np.mean(y))/np.std(y)\n",
    "    scores = -1*cross_validation.cross_val_score(regressor, X, y, cv=5, scoring = scoring_metric)\n",
    "    results = {'data': data_type, \"model\": estimator_name, \"Mean score\": np.mean(scores), \"STD score\": np.std(scores)}\n",
    "    return results\n",
    "\n",
    "for biomarker in biomarkers:\n",
    "    print \"working on\", biomarker\n",
    "    summary_stats = []\n",
    "    for file_path in file_paths:\n",
    "        #\n",
    "        # data load and prep\n",
    "        #\n",
    "        df = pd.read_csv(file_path)\n",
    "        fields = list(df.columns)\n",
    "        microbiome_indx = [i for i, field in enumerate(fields) if \"__\" in field]\n",
    "        column_names = list(df.columns)\n",
    "        target_indx = [i for i, column_name in enumerate(column_names) if column_name == biomarker]\n",
    "        df_rel = df.iloc[:,target_indx + range(microbiome_indx[0], df.shape[1])]\n",
    "        df_rel[[biomarker]] = df_rel[[biomarker]].apply(lambda x: pd.to_numeric(x, errors = \"coerce\"))\n",
    "        df_clean = df_rel.dropna()\n",
    "        y = np.array(df_clean[biomarker])\n",
    "        X = np.array(df_clean.iloc[:,1:])\n",
    "        #\n",
    "        # Elastic Net\n",
    "        #\n",
    "        ENet = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], eps=0.001, n_alphas=100, normalize=False, \n",
    "                            max_iter=1000, tol=0.0001, cv=3, copy_X=True, n_jobs=-1)\n",
    "        summary_stats.append(CV_scores(ENet,X, y, data_type = file_path, estimator_name = \"Elastic Net\"))\n",
    "        #\n",
    "        # Linear SVM\n",
    "        # \n",
    "        linear_SVM = GridSearchCV(estimator=SVR(kernel='linear'), param_grid=dict(C=np.logspace(-2,10,20)), n_jobs = -1, \n",
    "                                  scoring = scoring_metric)\n",
    "        summary_stats.append(CV_scores(linear_SVM,X, y, data_type = file_path, estimator_name = \"Linear SVM\"))\n",
    "        #\n",
    "        # RBF SVM\n",
    "        # \n",
    "        RBF_SVM = GridSearchCV(estimator=SVR(kernel='rbf'), param_grid=dict(C=np.logspace(-2,10,20),gamma = np.logspace(-9, 3, 20)), n_jobs = -1, \n",
    "                               scoring = scoring_metric)\n",
    "        summary_stats.append(CV_scores(RBF_SVM,X, y, data_type = file_path, estimator_name = \"RBF SVM\"))\n",
    "        #    \n",
    "        # RF\n",
    "        #\n",
    "        RF = RandomForestRegressor(n_estimators=1000, n_jobs=-1)\n",
    "        summary_stats.append(CV_scores(RF,X, y, data_type = file_path, estimator_name = \"RF\"))\n",
    "        #\n",
    "        # KNN \n",
    "        #\n",
    "        KNN = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=dict(n_neighbors=range(1,11), p=[1,2]), n_jobs = -1, \n",
    "                           scoring = scoring_metric)        \n",
    "        summary_stats.append(CV_scores(KNN,X, y, data_type = file_path, estimator_name = \"KNN\"))\n",
    "    \n",
    "    results_df = pd.DataFrame(summary_stats)\n",
    "    results_df.to_csv(\"../results/LS/\" + biomarker + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get RF important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Stool.Lysozyme\n",
      "working on Stool.Lactoferrin\n",
      "working on Stool.SIgA\n",
      "working on X..SCFA.Acetate\n",
      "working on X..SCFA.Propionate\n",
      "working on X..SCFA.Valerate\n",
      "working on X..SCFA.Butyrate\n",
      "working on Total.SCFA\n",
      "working on Butyrate\n",
      "working on Stool.pH\n",
      "working on Neutrophil.Count\n",
      "working on Lymphocyte.Count\n",
      "working on Monocyte.Count\n",
      "working on Esoinophil.Count\n"
     ]
    }
   ],
   "source": [
    "for biomarker in biomarkers:\n",
    "    print \"working on\", biomarker\n",
    "    for file_path in file_paths:\n",
    "        #\n",
    "        # data load and prep\n",
    "        #\n",
    "        df = pd.read_csv(file_path)\n",
    "        fields = list(df.columns)\n",
    "        microbiome_indx = [i for i, field in enumerate(fields) if \"__\" in field]\n",
    "        column_names = list(df.columns)\n",
    "        target_indx = [i for i, column_name in enumerate(column_names) if column_name == biomarker]\n",
    "        df_rel = df.iloc[:,target_indx + range(microbiome_indx[0], df.shape[1])]\n",
    "        df_rel[[biomarker]] = df_rel[[biomarker]].apply(lambda x: pd.to_numeric(x, errors = \"coerce\"))\n",
    "        df_clean = df_rel.dropna()\n",
    "        y = np.array(df_clean[biomarker])\n",
    "        X = np.array(df_clean.iloc[:,1:])\n",
    "\n",
    "        kf = KFold(len(y), n_folds=10)\n",
    "        important_features = []\n",
    "        for train, test in kf:\n",
    "            X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "            RF = RandomForestRegressor(n_estimators=1000, n_jobs=-1)\n",
    "            RF.fit(X_train, y_train)\n",
    "            important_features.append(RF.feature_importances_)\n",
    "        \n",
    "        importance_df = pd.DataFrame({'microbe': column_names[microbiome_indx[0]:], \n",
    "              'avg.importance': np.mean(np.array(important_features), axis = 0),\n",
    "              'hmean.importance': hmean(np.array(important_features)+1e-22),\n",
    "              'median.importance': np.median(np.array(important_features), axis = 0)})\n",
    "        tax_level = file_path.split(\"LS-\")[-1].split(\"-\")[0]\n",
    "        importance_df.to_csv(\"../results/LS/\"+tax_level+\"_importance_\" + biomarker + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
